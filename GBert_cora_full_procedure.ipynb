{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d99aaa",
   "metadata": {},
   "source": [
    "# Load Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ebef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_sparse/_convert_cpu.so, 0x0006): Symbol not found: __ZN2at8internal15invoke_parallelExxxRKNSt3__18functionIFvxxEEE\n",
      "  Referenced from: <10480BD5-33A4-3A5E-8C3D-8961DBA73F4E> /opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch_sparse/_convert_cpu.so\n",
      "  Expected in:     <616791F0-29C3-3F88-8A88-D072E7E40979> /opt/anaconda3/envs/Python_interpre/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "content_path = './data/Cora/cora.content'\n",
    "cites_path = './data/Cora/cora.cites'\n",
    "\n",
    "content_df = pd.read_csv(content_path, sep='\\t', header=None)\n",
    "\n",
    "paper_ids = content_df[0].tolist()  \n",
    "features = torch.tensor(content_df.iloc[:, 1:-1].values, dtype=torch.float) \n",
    "labels_raw = content_df.iloc[:, -1].tolist()  \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = torch.tensor(label_encoder.fit_transform(labels_raw), dtype=torch.long)\n",
    "\n",
    "id_map = {pid: i for i, pid in enumerate(paper_ids)}\n",
    "\n",
    "cites_df = pd.read_csv(cites_path, sep='\\t', header=None, names=['source', 'target'])\n",
    "\n",
    "cites_df = cites_df[cites_df['source'].isin(id_map) & cites_df['target'].isin(id_map)]\n",
    "\n",
    "src = cites_df['source'].map(id_map).tolist()\n",
    "dst = cites_df['target'].map(id_map).tolist()\n",
    "\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "data_raw = Data(x=features, edge_index=edge_index, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9968651",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw # data = data_preprocess\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "########## one-hot encoding\n",
    "num_classes = len(data.y.unique())  # Cora has 7 classes\n",
    "\n",
    "# F.one_hot for one-hot coding\n",
    "y_one_hot = F.one_hot(data.y, num_classes=num_classes).float()  # cora dataset shape: [2708, 7] or ABIDE dataset shape: [270000, 2]\n",
    "\n",
    "data.num_nodes = data.x.shape[0]\n",
    "data.node_list = list(range(data.num_nodes))  # initial nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c375",
   "metadata": {},
   "source": [
    "# intimacy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99276cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/jlhdr74n64dfb6pm21lq8x180000gn/T/ipykernel_16624/606168343.py:33: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e3pikzc5fh/croot/libtorch_1738337599132/work/torch/csrc/utils/tensor_new.cpp:653.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import inv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "########## one-hot encoding\n",
    "num_classes = len(data.y.unique())  # Cora has 7 classes\n",
    "\n",
    "# F.one_hot for one-hot coding\n",
    "y_one_hot = F.one_hot(data.y, num_classes=num_classes).float()  # cora dataset shape: [2708, 7] or ABIDE dataset shape: [270000, 2]\n",
    "\n",
    "\n",
    "# building the adjacency matrix\n",
    "def adj_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  # Sum over rows (degrees of nodes)\n",
    "    r_inv = np.power(rowsum, -0.5).flatten()  # Inverse square root of degrees\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # Handle division by zero (e.g., for isolated nodes)\n",
    "    r_mat_inv = sp.diags(r_inv)  # Create a sparse diagonal matrix from the inverse degrees\n",
    "    mx = r_mat_inv.dot(mx).dot(r_mat_inv)  # Apply the normalization formula\n",
    "    return mx\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "edges_index = data.edge_index.cpu().numpy().T  # [num_edges, 2]\n",
    "adj = sp.coo_matrix(\n",
    "    (np.ones(edges_index.shape[0]), (edges_index[:, 0], edges_index[:, 1])),\n",
    "    shape=(y_one_hot.shape[0], y_one_hot.shape[0]),\n",
    "    dtype=np.float32\n",
    ")\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "norm_adj = adj_normalize(adj + sp.eye(adj.shape[0]))# normalized adjacency matrix\n",
    "\n",
    "data.adj = sparse_mx_to_torch_sparse_tensor(norm_adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fbdbd",
   "metadata": {},
   "source": [
    "# Weisfeiler-Lehman (WL) based graph coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac69e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WLGraphColoring:\n",
    "    def __init__(self, max_iter=2):\n",
    "        self.max_iter = max_iter\n",
    "        self.node_color_dict = {}\n",
    "        self.node_neighbor_dict = {}\n",
    "\n",
    "    def setting_init(self, node_list, edge_index):\n",
    "        for node in node_list:\n",
    "            self.node_color_dict[node] = 1\n",
    "            self.node_neighbor_dict[node] = {}\n",
    "\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            u1, u2 = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            self.node_neighbor_dict[u1][u2] = 1\n",
    "            self.node_neighbor_dict[u2][u1] = 1\n",
    "\n",
    "    def WL_recursion(self, node_list):\n",
    "        iteration_count = 1\n",
    "        while True:\n",
    "            new_color_dict = {}\n",
    "            for node in node_list:\n",
    "                neighbors = self.node_neighbor_dict[node]\n",
    "                neighbor_color_list = [self.node_color_dict[neb] for neb in neighbors]\n",
    "                color_string_list = [str(self.node_color_dict[node])] + sorted([str(color) for color in neighbor_color_list])\n",
    "                color_string = \"_\".join(color_string_list)\n",
    "                hash_object = hashlib.md5(color_string.encode())\n",
    "                hashing = hash_object.hexdigest()  # Using MD5 hash function\n",
    "                new_color_dict[node] = hashing\n",
    "\n",
    "            color_index_dict = {k: v + 1 for v, k in enumerate(sorted(set(new_color_dict.values())))}\n",
    "            for node in new_color_dict:\n",
    "                new_color_dict[node] = color_index_dict[new_color_dict[node]]\n",
    "\n",
    "            if self.node_color_dict == new_color_dict or iteration_count == self.max_iter:\n",
    "                return  \n",
    "            else:\n",
    "                self.node_color_dict = new_color_dict\n",
    "            iteration_count += 1\n",
    "\n",
    "    def save_coloring(self, save_path):\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.node_color_dict, f)\n",
    "\n",
    "    def visualize_graph(self):\n",
    "        G = nx.Graph()\n",
    "        for node, neighbors in self.node_neighbor_dict.items():\n",
    "            for neighbor in neighbors:\n",
    "                G.add_edge(node, neighbor)\n",
    "\n",
    "        node_colors = [self.node_color_dict[node] for node in G.nodes()]\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        nx.draw(G, node_color=node_colors, with_labels=True, cmap=plt.cm.viridis)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "wl_coloring = WLGraphColoring()\n",
    "\n",
    "# Assuming `data.node_list` and `data.edge_index` are your node list and edge index\n",
    "wl_coloring.setting_init(data.node_list, data.edge_index)\n",
    "wl_coloring.WL_recursion(data.node_list)\n",
    "\n",
    "# Save the node color dict\n",
    "saving_path = './results'\n",
    "os.makedirs(os.path.dirname(f\"{saving_path}/WL/WL\"), exist_ok=True)\n",
    "\n",
    "wl_coloring.save_coloring(f'{saving_path}/WL/WL')\n",
    "\n",
    "# Visualize the graph coloring\n",
    "# wl_coloring.visualize_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278a0e5",
   "metadata": {},
   "source": [
    "# Top-k Personalized PageRank neighbor for propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b77e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# PageRank-inspired adjacent matrix for propogation\n",
    "def neumann_approx_inverse(adj, c=0.15, K=10):\n",
    "    \"\"\"\n",
    "    c = 0.15\n",
    "    Neumann series approximation for eigen_adj = 0.15 * inv(I - (1 - 0.15) * A)\n",
    "    A is the normalized adjacency matrix\n",
    "    Returns a sparse matrix\n",
    "    \"\"\"\n",
    "    alpha = 1 - c\n",
    "    A = adj_normalize(adj)\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # initial value is ones matrix\n",
    "    result = sp.eye(n, format='csr', dtype=np.float32)\n",
    "    A_power = sp.eye(n, format='csr', dtype=np.float32)  # A^0\n",
    "\n",
    "    for k in range(1, K + 1):\n",
    "        A_power = alpha * A.dot(A_power)  # A^k\n",
    "        result += A_power\n",
    "\n",
    "    return c * result  # c * sum(alpha^k A^k)\n",
    "\n",
    "\n",
    "#Sparse version\n",
    "# def get_top_k_sparse(eigen_adj: sp.spmatrix, k: int):\n",
    "def get_top_k_sparse(adj: sp.spmatrix, c: float, k: int):\n",
    "    \"\"\"\n",
    "    eigen_adj: scipy.sparse.csr_matrix or coo_matrix\n",
    "    k: int\n",
    "    return: dict[node] = [(neighbor, score), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    # PageRank-inspired adjacent matrix for propogation\n",
    "    eigen_adj = neumann_approx_inverse(adj, c=c, K=15)\n",
    "    \n",
    "    dense = eigen_adj.toarray()\n",
    "    n = dense.shape[0]\n",
    "    result_dict = {}\n",
    "    \n",
    "    key_map = None\n",
    "    for i in range(n):\n",
    "        scores = dense[i]\n",
    "        scores[i] = -np.inf # remove self connection\n",
    "        if k < n:\n",
    "            top_k_idx = np.argpartition(-scores, k)[:k]\n",
    "        else:\n",
    "            top_k_idx = np.arange(n)\n",
    "            top_k_idx = top_k_idx[top_k_idx != i]\n",
    "        \n",
    "        # sorting\n",
    "        top_k_idx = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "\n",
    "        neighbors = [(idx, scores[idx]) for idx in top_k_idx]\n",
    "        mapped_node = key_map.get(i, i) if key_map else i\n",
    "        mapped_neighbors = [(key_map.get(nid, nid) if key_map else nid, val) for nid, val in neighbors]\n",
    "\n",
    "        result_dict[mapped_node] = mapped_neighbors\n",
    "\n",
    "        \n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "# KBatch = get_top_k_sparse(adj, 0.15, 2)# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9d902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def BatchHopDistance(node_list, edge_index, k, batch_path):\n",
    "    import pickle\n",
    "    import networkx as nx\n",
    "    \n",
    "    edge_index = edge_index.cpu().numpy()\n",
    "    link_list = list(zip(edge_index[0], edge_index[1]))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(link_list)\n",
    "\n",
    "        \n",
    "\n",
    "    with open(f\"{batch_path}/Batch/top_{k}_GraphBatching\", 'rb') as f:\n",
    "        batch_dict = pickle.load(f)\n",
    "\n",
    "    hop_dict = {}\n",
    "\n",
    "    for node in batch_dict:\n",
    "        try:\n",
    "            node_hop_lengths = nx.single_source_shortest_path_length(G, node, cutoff=10)\n",
    "        except:\n",
    "            node_hop_lengths = {}\n",
    "\n",
    "        hop_dict[node] = {}\n",
    "        for neighbor, _ in batch_dict[node]:\n",
    "            hop = node_hop_lengths.get(neighbor, 99)\n",
    "            hop_dict[node][neighbor] = hop\n",
    "\n",
    "    return hop_dict\n",
    "\n",
    "##--------------------------------------------------------\n",
    "# you can define how many neighbors \n",
    "# for k in [1,2,3,4,5,6,7,8]:\n",
    "# for k in [40,50,60,70,80,90,100,110, 120, 130, 140, 150]:\n",
    "for k in [7]:\n",
    "    \n",
    "    # KBatch = get_top_k_sparse(adj, 0.15, k)\n",
    "    KBatch = get_top_k_sparse(adj, 0.15, k)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(f\"{saving_path}/Batch/top_{k}_GraphBatching\"), exist_ok=True)\n",
    "    f = open(f\"{saving_path}/Batch/top_{k}_GraphBatching\", 'wb')\n",
    "    pickle.dump(KBatch, f)\n",
    "    f.close()\n",
    "    \n",
    "    KHop = BatchHopDistance(data.node_list, data.edge_index, k, saving_path)\n",
    "    os.makedirs(os.path.dirname(f\"{saving_path}/Hop/top_{k}_GraphBatchingHop\"), exist_ok=True)\n",
    "    f = open(f\"{saving_path}/Hop/top_{k}_GraphBatchingHop\", 'wb')\n",
    "    pickle.dump(KHop, f)\n",
    "    f.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b82f37",
   "metadata": {},
   "source": [
    "# Create embedding for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ae4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load WL Dictionary\n",
      "Load Hop Distance Dictionary\n",
      "Load Subgraph Batches\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "\n",
    "def load_hop_wl_batch(save_dir,k):\n",
    "    print('Load WL Dictionary')\n",
    "    f = open(f'{save_dir}/WL/WL', 'rb')\n",
    "    # f = open(f'{save_dir}/WL/cora', 'rb')\n",
    "    wl_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print('Load Hop Distance Dictionary')\n",
    "    f = open(f'{save_dir}/Hop/top_{k}_GraphBatchingHop', 'rb')\n",
    "    # f = open(f'{save_dir}/Hop/hop_cora_{k}', 'rb')\n",
    "    hop_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    print('Load Subgraph Batches')\n",
    "    f = open(f'{save_dir}/Batch/top_{k}_GraphBatching', 'rb')\n",
    "    # f = open(f'{save_dir}/Batch/cora_{k}', 'rb')\n",
    "    batch_dict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return hop_dict, wl_dict, batch_dict\n",
    "\n",
    "# Main functions\n",
    "embedding_dimension = 7\n",
    "saving_path = './results'\n",
    "hop_dict, wl_dict, batch_dict = load_hop_wl_batch(saving_path,embedding_dimension)\n",
    "\n",
    "# adj = sparse_mx_to_torch_sparse_tensor(norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf20039",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feature_list = []\n",
    "role_ids_list = []\n",
    "position_ids_list = []\n",
    "hop_ids_list = []\n",
    "idx = data.node_list\n",
    "\n",
    "for node in idx:\n",
    "    node_index = node \n",
    "    neighbors_list = batch_dict[node]\n",
    "\n",
    "    raw_feature = [data.x[node_index].tolist()]\n",
    "    role_ids = [wl_dict[node]]\n",
    "    position_ids = range(len(neighbors_list) + 1)\n",
    "    hop_ids = [0]\n",
    "    for neighbor, intimacy_score in neighbors_list:\n",
    "        neighbor_index = neighbor\n",
    "        \n",
    "        raw_feature.append(data.x[neighbor_index].tolist())\n",
    "        role_ids.append(wl_dict[neighbor])\n",
    "        if neighbor in hop_dict[node]:\n",
    "            hop_ids.append(hop_dict[node][neighbor])\n",
    "        else:\n",
    "            hop_ids.append(99)\n",
    "    raw_feature_list.append(raw_feature)\n",
    "    role_ids_list.append(role_ids)\n",
    "    position_ids_list.append(position_ids)\n",
    "    hop_ids_list.append(hop_ids)\n",
    "\n",
    "raw_embeddings = torch.FloatTensor(raw_feature_list)\n",
    "wl_embedding = torch.LongTensor(role_ids_list)\n",
    "hop_embeddings = torch.LongTensor(hop_ids_list)\n",
    "int_embeddings = torch.LongTensor(position_ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedb211",
   "metadata": {},
   "source": [
    "# Bert embedding and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82a2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/Python_interpre/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Concrete MethodModule class for a specific learning MethodModule\n",
    "'''\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, residual_h=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            #---- add residual ----\n",
    "            if residual_h is not None:\n",
    "                for index in range(hidden_states.size()[1]):\n",
    "                    hidden_states[:,index,:] += residual_h\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from features, wl, position and hop vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.raw_feature_embeddings = nn.Linear(config.x_size, config.hidden_size)\n",
    "        self.wl_role_embeddings = nn.Embedding(config.max_wl_role_index, config.hidden_size)\n",
    "        self.inti_pos_embeddings = nn.Embedding(config.max_inti_pos_index, config.hidden_size)\n",
    "        self.hop_dis_embeddings = nn.Embedding(config.max_hop_dis_index, config.hidden_size)\n",
    "        # self.attr_dis_embeddings = nn.Linear(1, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, raw_features=None, wl_role_ids=None, init_pos_ids=None, hop_dis_ids=None, attr_ids=None):\n",
    "\n",
    "        raw_feature_embeds = self.raw_feature_embeddings(raw_features)\n",
    "        # raw_feature_embeds_drop = self.dropout(raw_feature_embeds)\n",
    "        \n",
    "        role_embeddings = self.wl_role_embeddings(wl_role_ids)\n",
    "        # role_embeddings_drop = self.dropout(role_embeddings)\n",
    "        \n",
    "        position_embeddings = self.inti_pos_embeddings(init_pos_ids)\n",
    "        # position_embeddings_drop = self.dropout(position_embeddings)\n",
    "        \n",
    "        hop_embeddings = self.hop_dis_embeddings(hop_dis_ids)\n",
    "        # hop_embeddings_drop = self.dropout(hop_embeddings)\n",
    "        \n",
    "        # attr_embeddings = self.attr_dis_embeddings(attr_ids.unsqueeze(-1))\n",
    "        # attr_embeddings_drop = self.dropout(attr_embeddings)\n",
    "\n",
    "        #---- here, we use summation ----\n",
    "        # embeddings = raw_feature_embeds + role_embeddings + position_embeddings + hop_embeddings + attr_embeddings\n",
    "        embeddings = raw_feature_embeds + role_embeddings + position_embeddings + hop_embeddings\n",
    "        # embeddings = raw_feature_embeds_drop + role_embeddings_drop + position_embeddings_drop + hop_embeddings_drop + attr_embeddings_drop\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class NodeConstructOutputLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NodeConstructOutputLayer, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.x_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.x_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder:\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af62057",
   "metadata": {},
   "source": [
    "# Graph Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1003be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concrete MethodModule class for a specific learning MethodModule\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertPooler\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class MethodGraphBert(BertPreTrainedModel):\n",
    "    data = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBert, self).__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, head_mask=None, residual_h=None):\n",
    "        if head_mask is None:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        embedding_output = self.embeddings(raw_features=raw_features, wl_role_ids=wl_role_ids, init_pos_ids=init_pos_ids, \n",
    "                                        hop_dis_ids=hop_dis_ids)\n",
    "        encoder_outputs = self.encoder(embedding_output, head_mask=head_mask, residual_h=residual_h)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]\n",
    "        \n",
    "        # return outputs, embedding_output, encoder_outputs, sequence_output, pooled_output # Test Output \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98c0c1",
   "metadata": {},
   "source": [
    "# GraphBert Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401371c",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2227cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora config\n",
    "nclass = len(data.y.unique()) # for cora dataset\n",
    "nfeature = data.x.shape[1]\n",
    "ngraph = data.x.shape[0]\n",
    "\n",
    "#\n",
    "x_size = nfeature\n",
    "y_size = nclass\n",
    "graph_size = ngraph\n",
    "residual_type = 'graph_raw'\n",
    "# residual_type = 'raw'\n",
    "# residual_type = 'none'## good performances\n",
    "\n",
    "#Bert Config\n",
    "max_wl_role_index = 100\n",
    "max_hop_dis_index = 100\n",
    "max_attr_dis_index = embedding_dimension+1\n",
    "max_inti_pos_index = 100\n",
    "residual_type = residual_type\n",
    "x_size = x_size\n",
    "y_size = y_size\n",
    "k = nclass#Embedding dimension\n",
    "\n",
    "\n",
    "# Network setting\n",
    "hidden_size = 32 #32\n",
    "num_hidden_layers = 1 #2\n",
    "num_attention_heads = 4 #2\n",
    "hidden_act = 'gelu'\n",
    "intermediate_size = 128 #32: 2*hidden_size\n",
    "hidden_dropout_prob = 0.2#0.5\n",
    "attention_probs_dropout_prob = 0.2#0.3\n",
    "initializer_range = 0.02\n",
    "layer_norm_eps = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f800ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass  # simple container\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Input embedding\n",
    "config.x_size = x_size\n",
    "config.hidden_size = hidden_size\n",
    "config.max_wl_role_index = max_wl_role_index\n",
    "config.max_inti_pos_index = max_inti_pos_index\n",
    "config.max_hop_dis_index = max_hop_dis_index\n",
    "# config.max_attr_dis_index = max_attr_dis_index\n",
    "config.layer_norm_eps = layer_norm_eps\n",
    "config.hidden_dropout_prob = hidden_dropout_prob\n",
    "\n",
    "# Encoder\n",
    "config.output_attentions = False\n",
    "config.output_hidden_states = False\n",
    "config.num_hidden_layers = num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBertConfig(PretrainedConfig):\n",
    "    # default values\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_type = 'none',\n",
    "        x_size=3000,\n",
    "        y_size=7,\n",
    "        k=5,\n",
    "        max_wl_role_index = 100,\n",
    "        max_hop_dis_index = 100,\n",
    "        max_inti_pos_index = 100,\n",
    "        # max_attr_dis_index = 100,\n",
    "        hidden_size=32,#32,\n",
    "        num_hidden_layers=1,\n",
    "        num_attention_heads=1,\n",
    "        intermediate_size=32,#32,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.5,#0.5,\n",
    "        attention_probs_dropout_prob=0.5,#0.3,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        is_decoder=False,\n",
    "        input_similarity = None,\n",
    "        input_feature = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GraphBertConfig, self).__init__(**kwargs)\n",
    "        self.max_wl_role_index = max_wl_role_index\n",
    "        self.max_hop_dis_index = max_hop_dis_index\n",
    "        self.max_inti_pos_index = max_inti_pos_index\n",
    "        # self.max_attr_dis_index = max_attr_dis_index\n",
    "        self.residual_type = residual_type\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.k = k\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.is_decoder = is_decoder\n",
    "        self.input_similarity = input_similarity\n",
    "        self.input_feature = input_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11f988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = GraphBertConfig(residual_type = residual_type, k=k, x_size=nfeature, y_size=y_size, \\\n",
    "                            hidden_size=hidden_size, intermediate_size=intermediate_size, \\\n",
    "                            num_attention_heads=num_attention_heads, \\\n",
    "                            num_hidden_layers=num_hidden_layers, \\\n",
    "                            input_similarity = data.adj, \\\n",
    "                            input_feature = data.x \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4005504",
   "metadata": {},
   "source": [
    "# Pre-training: Node reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d41cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodGraphBertNodeConstruct(BertPreTrainedModel):\n",
    "    learning_record_dict = {}\n",
    "    lr = 0.0001\n",
    "    weight_decay = 5e-4\n",
    "    max_epoch = 200\n",
    "    load_pretrained_path = ''\n",
    "    save_pretrained_path = ''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBertNodeConstruct, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.bert = MethodGraphBert(config)\n",
    "        self.cls_y = torch.nn.Linear(config.hidden_size, config.x_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, idx=None):\n",
    "\n",
    "        outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids)\n",
    "\n",
    "        sequence_output = 0\n",
    "        for i in range(self.config.k+1):\n",
    "            sequence_output += outputs[0][:,i,:]\n",
    "        sequence_output /= float(self.config.k+1)\n",
    "\n",
    "        x_hat = self.cls_y(sequence_output)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "GraphBertNodeConstruct = MethodGraphBertNodeConstruct(bert_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64782593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.0150 time: 0.1822s\n",
      "Epoch: 0011 loss_train: 0.0128 time: 0.1561s\n",
      "Epoch: 0021 loss_train: 0.0122 time: 0.1462s\n",
      "Epoch: 0031 loss_train: 0.0119 time: 0.1468s\n",
      "Epoch: 0041 loss_train: 0.0117 time: 0.1450s\n",
      "Epoch: 0051 loss_train: 0.0115 time: 0.1505s\n",
      "Epoch: 0061 loss_train: 0.0114 time: 0.1400s\n",
      "Epoch: 0071 loss_train: 0.0113 time: 0.1411s\n",
      "Epoch: 0081 loss_train: 0.0113 time: 0.1504s\n",
      "Epoch: 0091 loss_train: 0.0112 time: 0.1483s\n",
      "Epoch: 0101 loss_train: 0.0112 time: 0.1375s\n",
      "Epoch: 0111 loss_train: 0.0111 time: 0.1403s\n",
      "Epoch: 0121 loss_train: 0.0111 time: 0.1474s\n",
      "Epoch: 0131 loss_train: 0.0111 time: 0.1357s\n",
      "Epoch: 0141 loss_train: 0.0111 time: 0.1447s\n",
      "Epoch: 0151 loss_train: 0.0110 time: 0.1330s\n",
      "Epoch: 0161 loss_train: 0.0110 time: 0.1350s\n",
      "Epoch: 0171 loss_train: 0.0110 time: 0.1421s\n",
      "Epoch: 0181 loss_train: 0.0110 time: 0.1334s\n",
      "Epoch: 0191 loss_train: 0.0110 time: 0.1577s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 28.4450s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.44498610496521"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 0.0005\n",
    "max_epoch = 200\n",
    "\n",
    "node_learning_record_dict = {}\n",
    "\n",
    "t_begin = time.time()\n",
    "optimizer = optim.AdamW(GraphBertNodeConstruct.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "for epoch in range(max_epoch):\n",
    "    t_epoch_begin = time.time()\n",
    "\n",
    "    # -------------------------\n",
    "\n",
    "    GraphBertNodeConstruct.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = GraphBertNodeConstruct.forward(raw_embeddings, wl_embedding, int_embeddings, hop_embeddings)\n",
    "\n",
    "    # loss_train = F.mse_loss(output, data.x)\n",
    "    loss_train = F.mse_loss(output, data.x)\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    node_learning_record_dict[epoch] = {'loss_train': loss_train.item(), 'time': time.time() - t_epoch_begin}\n",
    "\n",
    "    # -------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "                'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                'time: {:.4f}s'.format(time.time() - t_epoch_begin))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_begin))\n",
    "time.time() - t_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb0c8a",
   "metadata": {},
   "source": [
    "# Fine-Tuning for the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcba3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import abc\n",
    "\n",
    "\n",
    "class evaluate:\n",
    "    \"\"\" \n",
    "    evaluate: Abstract Class\n",
    "    Entries: \n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_name = None\n",
    "    evaluate_description = None\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    # initialization function\n",
    "    def __init__(self, eName=None, eDescription=None):\n",
    "        self.evaluate_name = eName\n",
    "        self.evaluate_description = eDescription\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def evaluate(self):\n",
    "        return\n",
    "\n",
    "class EvaluateAcc(evaluate):\n",
    "    data = None\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \n",
    "        return accuracy_score(self.data['true_y'], self.data['pred_y'])\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class MethodGraphBertNodeClassification(BertPreTrainedModel):\n",
    "    learning_record_dict = {}\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    max_epoch = 500\n",
    "    spy_tag = True\n",
    "\n",
    "    load_pretrained_path = ''\n",
    "    save_pretrained_path = ''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MethodGraphBertNodeClassification, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.bert = MethodGraphBert(config)\n",
    "        self.res_h = torch.nn.Linear(config.x_size, config.hidden_size)\n",
    "        self.res_y = torch.nn.Linear(config.x_size, config.y_size)\n",
    "        self.cls_y = torch.nn.Linear(config.hidden_size, config.y_size)\n",
    "\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, data, idx=None):\n",
    "        \n",
    "        #Residual\n",
    "        residual_h, residual_y = self.residual_term()\n",
    "        \n",
    "        if idx is not None:\n",
    "            if residual_h is None:\n",
    "                outputs = self.bert(raw_features[idx], wl_role_ids[idx], init_pos_ids[idx], hop_dis_ids[idx], residual_h=None)\n",
    "            else:\n",
    "                outputs = self.bert(raw_features[idx], wl_role_ids[idx], init_pos_ids[idx], hop_dis_ids[idx], residual_h=residual_h[idx])\n",
    "                residual_y = residual_y[idx]\n",
    "            \n",
    "        else:\n",
    "            if residual_h is None:\n",
    "                outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, residual_h=None)\n",
    "            else:\n",
    "                outputs = self.bert(raw_features, wl_role_ids, init_pos_ids, hop_dis_ids, residual_h=residual_h)\n",
    "        \n",
    "        # Average the sequence output\n",
    "        sequence_output = 0\n",
    "        for i in range(self.config.k+1):\n",
    "            sequence_output += outputs[0][:,i,:]\n",
    "        sequence_output /= float(self.config.k+1)#opt1\n",
    "        # sequence_output = outputs[0].mean(dim=1)#opt2\n",
    "        \n",
    "        \n",
    "        labels = self.cls_y(sequence_output)\n",
    "\n",
    "        if residual_y is not None:\n",
    "            labels += residual_y\n",
    "\n",
    "        return F.log_softmax(labels, dim=1)\n",
    "        \n",
    "\n",
    "    def residual_term(self):\n",
    "        if self.config.residual_type == 'none':\n",
    "            return None, None\n",
    "        elif self.config.residual_type == 'raw':\n",
    "            return self.res_h(self.config.input_feature), self.res_y(self.config.input_feature)\n",
    "        elif self.config.residual_type == 'graph_raw':\n",
    "            return torch.spmm(self.config.input_similarity, self.res_h(self.config.input_feature)), torch.spmm(self.config.input_similarity, self.res_y(self.config.input_feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1b4e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MethodGraphBertNodeClassification(\n",
      "  (bert): MethodGraphBert(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (raw_feature_embeddings): Linear(in_features=1433, out_features=32, bias=True)\n",
      "      (wl_role_embeddings): Embedding(100, 32)\n",
      "      (inti_pos_embeddings): Embedding(100, 32)\n",
      "      (hop_dis_embeddings): Embedding(100, 32)\n",
      "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (res_h): Linear(in_features=1433, out_features=32, bias=True)\n",
      "  (res_y): Linear(in_features=1433, out_features=7, bias=True)\n",
      "  (cls_y): Linear(in_features=32, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "GraphBertNodeClassification = MethodGraphBertNodeClassification(bert_config)\n",
    "print(GraphBertNodeClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a131bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ------------------------------------\n",
    "# evaluation function\n",
    "def evaluate_in_batches(model, idx_eval, batch_size=10):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        \n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        for idx in idx_eval:\n",
    "            \n",
    "            \n",
    "            label_ouputs = model.forward(raw_embeddings, wl_embedding,\n",
    "                                int_embeddings, hop_embeddings, data, idx)\n",
    "            \n",
    "                \n",
    "            loss_train_cls_residual = F.cross_entropy(label_ouputs, data.y[idx])\n",
    "        \n",
    "            epoch_loss += loss_train_cls_residual.item() * len(data.y[idx])# based on the number of samples\n",
    "            \n",
    "            \n",
    "            #### examine the metrics\n",
    "            probs = F.softmax(label_ouputs, dim=1)  # shape: (batch, 2)\n",
    "            preds = torch.argmax(label_ouputs, dim=1)  # predicted class\n",
    "            \n",
    "            \n",
    "            all_probs.append(probs.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(data.y[idx].numpy())\n",
    "            \n",
    "            \n",
    "            epoch_correct += preds.eq(data.y[idx]).sum().item()\n",
    "            epoch_total += len(label_ouputs)\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        loss_train_avg = epoch_loss / epoch_total\n",
    "        acc_train_avg = epoch_correct / epoch_total\n",
    "        \n",
    "        # Calculate AUC\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        # all_probs = np.array(all_probs)\n",
    "        all_probs = np.concatenate(all_probs, axis=0)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        \n",
    "        roc_auc = roc_auc_score(\n",
    "            all_labels, all_probs, multi_class='ovr')\n",
    "        \n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds).ravel()\n",
    "            \n",
    "    return acc_train_avg, loss_train_avg, accuracy, precision, recall, f1, roc_auc, cm\n",
    "\n",
    "# ------------------------------------\n",
    "# early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0, mode='min', path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = None\n",
    "\n",
    "        if self.mode not in ['min', 'max']:\n",
    "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
    "\n",
    "    def __call__(self, val_metric, model, epoch=None):\n",
    "        score = val_metric if self.mode == 'min' else -val_metric\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(model, val_metric)\n",
    "        elif score < self.best_score - self.delta:  #  only when truly improved\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model, val_metric)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement. EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model, val_metric):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f\"Model improved. Saved to {self.path} | Val Metric: {val_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "  [Train] loss: 1.8634 | acc: 0.2571\n",
      "  [Valid] loss: 1.6863 | acc: 0.3433\n",
      "  [Test ] loss: 1.5790 | acc: 0.4290\n",
      "  Time: 0.1648s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 1.6863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002\n",
      "  [Train] loss: 1.2349 | acc: 0.6429\n",
      "  [Valid] loss: 1.4258 | acc: 0.5233\n",
      "  [Test ] loss: 1.2484 | acc: 0.5950\n",
      "  Time: 0.1788s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 1.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003\n",
      "  [Train] loss: 0.7296 | acc: 0.8929\n",
      "  [Valid] loss: 1.1098 | acc: 0.7000\n",
      "  [Test ] loss: 0.9195 | acc: 0.7660\n",
      "  Time: 0.1592s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 1.1098\n",
      "Epoch: 0004\n",
      "  [Train] loss: 0.3393 | acc: 0.9643\n",
      "  [Valid] loss: 0.9421 | acc: 0.7467\n",
      "  [Test ] loss: 0.7720 | acc: 0.7750\n",
      "  Time: 0.1632s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 0.9421\n",
      "Epoch: 0005\n",
      "  [Train] loss: 0.1703 | acc: 0.9857\n",
      "  [Valid] loss: 0.8709 | acc: 0.7667\n",
      "  [Test ] loss: 0.7260 | acc: 0.7800\n",
      "  Time: 0.1603s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 0.8709\n",
      "Epoch: 0006\n",
      "  [Train] loss: 0.0827 | acc: 0.9929\n",
      "  [Valid] loss: 0.8297 | acc: 0.7833\n",
      "  [Test ] loss: 0.6927 | acc: 0.7950\n",
      "  Time: 0.1666s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 0.8297\n",
      "Epoch: 0007\n",
      "  [Train] loss: 0.0366 | acc: 1.0000\n",
      "  [Valid] loss: 0.8042 | acc: 0.7967\n",
      "  [Test ] loss: 0.6697 | acc: 0.8130\n",
      "  Time: 0.1519s\n",
      "-----------------------------------------\n",
      "Model improved. Saved to ./results/checkpoint.pt | Val Metric: 0.8042\n",
      "Epoch: 0008\n",
      "  [Train] loss: 0.0184 | acc: 1.0000\n",
      "  [Valid] loss: 0.8407 | acc: 0.7633\n",
      "  [Test ] loss: 0.6924 | acc: 0.8100\n",
      "  Time: 0.1619s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 1/30\n",
      "Epoch: 0009\n",
      "  [Train] loss: 0.0157 | acc: 1.0000\n",
      "  [Valid] loss: 0.8670 | acc: 0.7600\n",
      "  [Test ] loss: 0.7047 | acc: 0.8150\n",
      "  Time: 0.1620s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 2/30\n",
      "Epoch: 0010\n",
      "  [Train] loss: 0.0093 | acc: 1.0000\n",
      "  [Valid] loss: 0.8644 | acc: 0.7700\n",
      "  [Test ] loss: 0.7014 | acc: 0.8180\n",
      "  Time: 0.1569s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 3/30\n",
      "Epoch: 0011\n",
      "  [Train] loss: 0.0043 | acc: 1.0000\n",
      "  [Valid] loss: 0.8625 | acc: 0.7667\n",
      "  [Test ] loss: 0.7025 | acc: 0.8170\n",
      "  Time: 0.1691s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 4/30\n",
      "Epoch: 0012\n",
      "  [Train] loss: 0.0031 | acc: 1.0000\n",
      "  [Valid] loss: 0.8611 | acc: 0.7600\n",
      "  [Test ] loss: 0.7050 | acc: 0.8190\n",
      "  Time: 0.1726s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 5/30\n",
      "Epoch: 0013\n",
      "  [Train] loss: 0.0029 | acc: 1.0000\n",
      "  [Valid] loss: 0.8612 | acc: 0.7733\n",
      "  [Test ] loss: 0.7088 | acc: 0.8190\n",
      "  Time: 0.1528s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 6/30\n",
      "Epoch: 0014\n",
      "  [Train] loss: 0.0025 | acc: 1.0000\n",
      "  [Valid] loss: 0.8612 | acc: 0.7733\n",
      "  [Test ] loss: 0.7098 | acc: 0.8180\n",
      "  Time: 0.1557s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 7/30\n",
      "Epoch: 0015\n",
      "  [Train] loss: 0.0027 | acc: 1.0000\n",
      "  [Valid] loss: 0.8609 | acc: 0.7733\n",
      "  [Test ] loss: 0.7105 | acc: 0.8180\n",
      "  Time: 0.1515s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 8/30\n",
      "Epoch: 0016\n",
      "  [Train] loss: 0.0024 | acc: 1.0000\n",
      "  [Valid] loss: 0.8604 | acc: 0.7767\n",
      "  [Test ] loss: 0.7105 | acc: 0.8190\n",
      "  Time: 0.1568s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 9/30\n",
      "Epoch: 0017\n",
      "  [Train] loss: 0.0024 | acc: 1.0000\n",
      "  [Valid] loss: 0.8602 | acc: 0.7733\n",
      "  [Test ] loss: 0.7102 | acc: 0.8190\n",
      "  Time: 0.1476s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 10/30\n",
      "Epoch: 0018\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8598 | acc: 0.7733\n",
      "  [Test ] loss: 0.7097 | acc: 0.8200\n",
      "  Time: 0.1525s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 11/30\n",
      "Epoch: 0019\n",
      "  [Train] loss: 0.0019 | acc: 1.0000\n",
      "  [Valid] loss: 0.8593 | acc: 0.7767\n",
      "  [Test ] loss: 0.7090 | acc: 0.8200\n",
      "  Time: 0.1608s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 12/30\n",
      "Epoch: 0020\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8591 | acc: 0.7767\n",
      "  [Test ] loss: 0.7086 | acc: 0.8200\n",
      "  Time: 0.1628s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 13/30\n",
      "Epoch: 0021\n",
      "  [Train] loss: 0.0018 | acc: 1.0000\n",
      "  [Valid] loss: 0.8588 | acc: 0.7767\n",
      "  [Test ] loss: 0.7080 | acc: 0.8200\n",
      "  Time: 0.1505s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 14/30\n",
      "Epoch: 0022\n",
      "  [Train] loss: 0.0018 | acc: 1.0000\n",
      "  [Valid] loss: 0.8584 | acc: 0.7767\n",
      "  [Test ] loss: 0.7074 | acc: 0.8190\n",
      "  Time: 0.1603s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 15/30\n",
      "Epoch: 0023\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8582 | acc: 0.7800\n",
      "  [Test ] loss: 0.7071 | acc: 0.8190\n",
      "  Time: 0.1570s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 16/30\n",
      "Epoch: 0024\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8580 | acc: 0.7800\n",
      "  [Test ] loss: 0.7068 | acc: 0.8180\n",
      "  Time: 0.1506s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 17/30\n",
      "Epoch: 0025\n",
      "  [Train] loss: 0.0019 | acc: 1.0000\n",
      "  [Valid] loss: 0.8578 | acc: 0.7800\n",
      "  [Test ] loss: 0.7066 | acc: 0.8180\n",
      "  Time: 0.1590s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 18/30\n",
      "Epoch: 0026\n",
      "  [Train] loss: 0.0017 | acc: 1.0000\n",
      "  [Valid] loss: 0.8577 | acc: 0.7800\n",
      "  [Test ] loss: 0.7065 | acc: 0.8180\n",
      "  Time: 0.1593s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 19/30\n",
      "Epoch: 0027\n",
      "  [Train] loss: 0.0020 | acc: 1.0000\n",
      "  [Valid] loss: 0.8576 | acc: 0.7800\n",
      "  [Test ] loss: 0.7064 | acc: 0.8180\n",
      "  Time: 0.1600s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 20/30\n",
      "Epoch: 0028\n",
      "  [Train] loss: 0.0019 | acc: 1.0000\n",
      "  [Valid] loss: 0.8575 | acc: 0.7800\n",
      "  [Test ] loss: 0.7063 | acc: 0.8180\n",
      "  Time: 0.1543s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 21/30\n",
      "Epoch: 0029\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8575 | acc: 0.7800\n",
      "  [Test ] loss: 0.7063 | acc: 0.8180\n",
      "  Time: 0.1584s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 22/30\n",
      "Epoch: 0030\n",
      "  [Train] loss: 0.0023 | acc: 1.0000\n",
      "  [Valid] loss: 0.8575 | acc: 0.7767\n",
      "  [Test ] loss: 0.7062 | acc: 0.8180\n",
      "  Time: 0.1582s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 23/30\n",
      "Epoch: 0031\n",
      "  [Train] loss: 0.0024 | acc: 1.0000\n",
      "  [Valid] loss: 0.8574 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8180\n",
      "  Time: 0.1539s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 24/30\n",
      "Epoch: 0032\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8574 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8180\n",
      "  Time: 0.1569s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 25/30\n",
      "Epoch: 0033\n",
      "  [Train] loss: 0.0023 | acc: 1.0000\n",
      "  [Valid] loss: 0.8574 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8180\n",
      "  Time: 0.1574s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 26/30\n",
      "Epoch: 0034\n",
      "  [Train] loss: 0.0020 | acc: 1.0000\n",
      "  [Valid] loss: 0.8573 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8190\n",
      "  Time: 0.1570s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 27/30\n",
      "Epoch: 0035\n",
      "  [Train] loss: 0.0023 | acc: 1.0000\n",
      "  [Valid] loss: 0.8573 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8190\n",
      "  Time: 0.1592s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 28/30\n",
      "Epoch: 0036\n",
      "  [Train] loss: 0.0021 | acc: 1.0000\n",
      "  [Valid] loss: 0.8573 | acc: 0.7767\n",
      "  [Test ] loss: 0.7061 | acc: 0.8190\n",
      "  Time: 0.1523s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 29/30\n",
      "Epoch: 0037\n",
      "  [Train] loss: 0.0023 | acc: 1.0000\n",
      "  [Valid] loss: 0.8573 | acc: 0.7767\n",
      "  [Test ] loss: 0.7060 | acc: 0.8190\n",
      "  Time: 0.1560s\n",
      "-----------------------------------------\n",
      "No improvement. EarlyStopping counter: 30/30\n",
      "Early stopping triggered.\n",
      "Optimization Finished!\n",
      "Total time elapsed: 5.8847s,     best testing performance  0.820000, minimun loss  0.669715\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Separate dataset for purpose, we only use limited dataset 140 nodes for training to test relatively big dataset\n",
    "idx_train = range(140)\n",
    "idx_test = range(200, 1200)\n",
    "idx_val = range(1200, 1500)\n",
    "\n",
    "\n",
    "classify_learning_record_dict = {}\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_loader = DataLoader(idx_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(idx_test, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(idx_val, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "params = []\n",
    "base_lr = 1e-3\n",
    "decay_factor = 0.9\n",
    "for i, (name, param) in enumerate(GraphBertNodeClassification.named_parameters()):\n",
    "    lr = base_lr * (decay_factor ** (len(list(GraphBertNodeClassification.named_children())) - i - 1))\n",
    "    params.append({'params': param, 'lr': lr})\n",
    "    \n",
    "# ------------------------------------\n",
    "max_epoch = 100\n",
    "\n",
    "\n",
    "t_begin = time.time()\n",
    "\n",
    "optimizer = optim.Adam(params, lr=base_lr, weight_decay=1e-4)\n",
    "\n",
    "accuracy = EvaluateAcc('', '')\n",
    "\n",
    "# initialization \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "\n",
    "classify_learning_record_dict = {}\n",
    "\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(patience=30, mode='min', path=f'./results/checkpoint.pt')\n",
    "\n",
    "\n",
    "max_score = 0.0\n",
    "for epoch in range(max_epoch):\n",
    "    t_epoch_begin = time.time()\n",
    "\n",
    "    GraphBertNodeClassification.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    \n",
    "    for load in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()            \n",
    "        \n",
    "        output  = GraphBertNodeClassification.forward(\n",
    "            raw_embeddings, wl_embedding, int_embeddings, hop_embeddings, data, idx=load)\n",
    "        \n",
    "        # Two loss functions\n",
    "        loss_train = F.cross_entropy(output, data.y[load])\n",
    "        \n",
    "        loss_train.backward()\n",
    "        \n",
    "        pred = (output).max(1)[1]\n",
    "        correct = pred.eq(data.y[load]).sum().item()\n",
    "\n",
    "        # epoch_loss += loss_train.item() * len(batch_idx)\n",
    "        epoch_loss += loss_train.item() * len(load)# based on the number of samples\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_correct += correct\n",
    "        \n",
    "        # epoch_total += len(batch_idx)\n",
    "        epoch_total += len(load)\n",
    "        \n",
    "        \n",
    "        # print(f\"Batch {i+1}/{num_batches}, Loss: {loss_train.item():.4f}, Accuracy: {correct/len(batch_idx):.4f}\")\n",
    "        \n",
    "\n",
    "    loss_train_avg = epoch_loss / epoch_total\n",
    "    acc_train_avg = epoch_correct / epoch_total\n",
    "\n",
    "    # evaluation\n",
    "    GraphBertNodeClassification.eval()#frozen\n",
    "    \n",
    "    # Validate (using mini-batch)\n",
    "    acc_val, loss_val, accuracy_val, precision_val, recall_val, f1_val, roc_auc_val, confusion_matrix_val = evaluate_in_batches(GraphBertNodeClassification, val_loader, batch_size=20)\n",
    "\n",
    "    # Test (using mini-batch)\n",
    "    acc_test, loss_test, accuracy_test, precision_test, recall_test, f1_test, roc_auc_test, confusion_matrix_test = evaluate_in_batches(GraphBertNodeClassification, test_loader, batch_size=20)\n",
    "\n",
    "    classify_learning_record_dict[epoch] = {\n",
    "        'loss_train': loss_train_avg,\n",
    "        'acc_train': acc_train_avg,\n",
    "        'loss_val': loss_val,\n",
    "        'acc_val': acc_val,\n",
    "        'loss_test': loss_test,\n",
    "        'acc_test': acc_test,\n",
    "        'time': time.time() - t_epoch_begin\n",
    "    }\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:04d}\")\n",
    "    print(f\"  [Train] loss: {loss_train_avg:.4f} | acc: {acc_train_avg:.4f}\")\n",
    "    print(f\"  [Valid] loss: {loss_val:.4f} | acc: {acc_val:.4f}\")\n",
    "    print(f\"  [Test ] loss: {loss_test:.4f} | acc: {acc_test:.4f}\")\n",
    "    print(f\"  Time: {time.time() - t_epoch_begin:.4f}s\")\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    #lr scheduler\n",
    "    scheduler.step(loss_val)\n",
    "    \n",
    "\n",
    "    early_stopping(loss_val, GraphBertNodeClassification)\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_begin) + ', \\\n",
    "    best testing performance {: 4f}'.format(np.max([classify_learning_record_dict[epoch]['acc_test'] for epoch in classify_learning_record_dict])) \\\n",
    "        + ', minimun loss {: 4f}'.format(np.min([classify_learning_record_dict[epoch]['loss_test'] for epoch in classify_learning_record_dict])))\n",
    "\n",
    "\n",
    "\n",
    "torch.save(classify_learning_record_dict, f'./results/classify_learning_record_dict.pth')\n",
    "    \n",
    "torch.save(GraphBertNodeClassification.state_dict(), './results/model_dict.pt')\n",
    "print(f\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a29566",
   "metadata": {},
   "source": [
    "# making the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "248f2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/jlhdr74n64dfb6pm21lq8x180000gn/T/ipykernel_16624/2323198184.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classify_learning_record_dict = torch.load(f'/Users/bobtian/github/GraphBert/results/classify_learning_record_dict.pth')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/bobtian/github/GraphBert/results/classify_learning_record_dict.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m classify_learning_record_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/bobtian/github/GraphBert/results/classify_learning_record_dict.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get the keys of the dictionary\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classify_learning_record_dict)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/Python_interpre/lib/python3.12/site-packages/torch/serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/bobtian/github/GraphBert/results/classify_learning_record_dict.pth'"
     ]
    }
   ],
   "source": [
    "classify_learning_record_dict = torch.load(f'/Users/bobtian/github/GraphBert/results/classify_learning_record_dict.pth')\n",
    "\n",
    "# Get the keys of the dictionary\n",
    "for key in range(len(classify_learning_record_dict)):\n",
    "    df = pd.DataFrame.from_dict(classify_learning_record_dict[key], orient='index')\n",
    "    df = df.sort_index()\n",
    "    \n",
    "#     ## average the values across all indices\n",
    "#     all_series.append(df.mean(axis=0))\n",
    "    \n",
    "# all_series = pd.DataFrame(all_series)\n",
    "# all_series.rename(columns={'accuracy_test': f'accuracy_test_{i}'}, inplace=True)\n",
    "# all_series.rename(columns={'loss_test': f'loss_test_{i}'}, inplace=True)\n",
    "# all_series.rename(columns={'time': f'time_{i}'}, inplace=True)\n",
    "\n",
    "# result_df_acc_test.append( all_series[f'accuracy_test_{i}'] )\n",
    "# result_df_loss_test.append( all_series[f'loss_test_{i}'] )\n",
    "# result_df_time_test.append( all_series[f'time_{i}'] )\n",
    "\n",
    "# result_df_acc_test = pd.DataFrame(result_df_acc_test) \n",
    "# result_df_loss_test = pd.DataFrame(result_df_loss_test) \n",
    "# result_df_time_test = pd.DataFrame(result_df_time_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_training=[]\n",
    "loss_val=[]\n",
    "\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "acc_test = []\n",
    "\n",
    "for key in classify_learning_record_dict:\n",
    "    df = pd.DataFrame.from_dict(classify_learning_record_dict[key], orient='index')\n",
    "    loss_training.append(df.loc['loss_train'].values[0])\n",
    "    loss_val.append(df.loc['loss_val'].values[0])\n",
    "    \n",
    "    acc_train.append(df.loc['acc_train'].values[0])\n",
    "    acc_test.append(df.loc['acc_test'].values[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(loss_training) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, loss_training, 'o-', label='Train Loss')\n",
    "plt.plot(epochs, loss_val, 'o-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curve')\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, acc_train, 'o-', label='Train Accuracy')\n",
    "plt.plot(epochs, acc_test, 'o-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_interpre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
